{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Multimodal RAG Pipeline over an Auto Insurance Claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cookbook shows how to use LlamaParse to parse an auto insurance claim document that contains complex tabular data using OpenAI's multimodal GPT-4o model.\n",
    "\n",
    "This example demonstrates how LlamaParse can be used on insurance documents, which often contains complex tabular data. We parse these tabluar PDF files into markdown-formatted tables, which can be indexed and queried over with a `VectorStoreIndex`. This can help insurance companies accelerate the process of gathering information about a particular accident from insurance claim documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Setup\n",
    "\n",
    "Install LlamaIndex, download the data, and apply `nest_asyncio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/user-attachments/files/16435705/claim.pdf -O claim.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your OpenAI and LlamaCloud keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<Your OpenAI API Key>\"\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"<Your LlamaCloud API Key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up LlamaParse. We want to parse the PDF file into markdown, translating the tabular data into markdown tables. To ensure accuracy, we will use the GPT-4o multimodal model to parse the PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\",\n",
    "    parsing_instruction=\"This is an auto insurance claim document.\",\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"openai-gpt4o\",\n",
    ")\n",
    "\n",
    "md_json_objs = parser.get_json_result(\n",
    "    \"claim.pdf\"\n",
    ")  # extract markdown data for insurance claim document\n",
    "md_json_list = md_json_objs[0][\"pages\"]  # extract list of pages for insurance claim doc\n",
    "parser.get_images(\n",
    "    md_json_objs, download_path=\"data_images\"\n",
    ")  # extract images from PDFs and save them to ./data_images/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions to create a list of `TextNode`s to feed into the `VectorStoreIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import typing as t\n",
    "from llama_index.core.schema import TextNode, ImageNode\n",
    "\n",
    "\n",
    "def get_page_number(file_name):\n",
    "    \"\"\"Gets page number of images using regex on file names\"\"\"\n",
    "    match = re.search(r\"-page-(\\d+)\\.jpg$\", str(file_name))\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 0\n",
    "\n",
    "\n",
    "def _get_sorted_image_files(image_dir):\n",
    "    \"\"\"Get image files sorted by page.\"\"\"\n",
    "    raw_files = [f for f in list(Path(image_dir).iterdir()) if f.is_file()]\n",
    "    sorted_files = sorted(raw_files, key=get_page_number)\n",
    "    return sorted_files\n",
    "\n",
    "\n",
    "def get_text_nodes(json_dicts, image_dir) -> t.List[TextNode]:\n",
    "    \"\"\"Creates nodes from json + images\"\"\"\n",
    "\n",
    "    nodes = []\n",
    "\n",
    "    docs = [doc[\"md\"] for doc in json_dicts]  # extract text\n",
    "    image_files = _get_sorted_image_files(image_dir)  # extract images\n",
    "\n",
    "    for idx, doc in enumerate(docs):\n",
    "        # adds both a text node and the corresponding image node (jpg of the page) for each page\n",
    "        node = TextNode(\n",
    "            text=doc,\n",
    "            metadata={\"image_path\": str(image_files[idx]), \"page_num\": idx + 1},\n",
    "        )\n",
    "        image_node = ImageNode(\n",
    "            image_path=str(image_files[idx]),\n",
    "            metadata={\"page_num\": idx + 1, \"text_node_id\": node.id_},\n",
    "        )\n",
    "        nodes.extend([node, image_node])\n",
    "\n",
    "    return nodes\n",
    "\n",
    "\n",
    "text_nodes = get_text_nodes(md_json_list, \"data_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "llm = OpenAI(\"gpt-4o\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "if not os.path.exists(\"storage_nodes\"):\n",
    "    index = VectorStoreIndex(text_nodes, embed_model=embed_model)\n",
    "    index.storage_context.persist(persist_dir=\"./storage_insurance\")\n",
    "else:\n",
    "    ctx = StorageContext.from_defaults(persist_dir=\"./storage_insurance\")\n",
    "    index = load_index_from_storage(ctx)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example queries are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Michael De Santa filed the insurance claim."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = query_engine.query(\"Who filed the insurance claim?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The accident happened at the intersection of Eclipse Boulevard and Marlow Drive in Los Angeles, CA."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Where did the accident happen?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The red sedan sustained significant damage in the car accident. The initial collision with the blue pickup truck in the intersection caused substantial front-end damage, including crumpled components like the hood, fenders, and bumper. The force of this impact then caused the sedan to spin and strike a nearby traffic pole, further damaging the side and rear of the vehicle. The combination of these two collisions resulted in severe structural damage, likely compromising the frame, suspension, and other critical systems, rendering the vehicle inoperable."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"How was the red sedan damaged?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The blue pickup had three occupants: Walter White, Skylar White, and Walter White Jr."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Who was in the blue pickup?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The blue pickup is owned by Walter White."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Who owns the blue pickup?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "One of the witnesses is Franklin Clinton. He can be contacted at 3671 Whispy Mound Drive, Los Angeles, CA 90068, with the phone number 3285550156."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"Who are some witnesses and how can we contact them?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Michael De Santa is likely liable for the damages. The reasoning is based on the description of the accident, which indicates that Michael De Santa, driving a red sedan, collided with a blue pickup truck at an intersection. The severity of the impact and the subsequent collision with a traffic pole suggest that Michael De Santa may have been at fault, leading to significant damage to both vehicles."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "response = chat_engine.chat(\n",
    "    \"Given the context, name a party that is liable for the damages and provide reasoning.\"\n",
    ")\n",
    "display(Markdown(str(response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-parse-5ZmnAQ0r-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
